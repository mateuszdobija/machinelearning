{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quality metrics for NLP\n",
    "\n",
    "There are several levels of chatbot quality measurements. In this section start with the most backend measures related strictly to the machine learning models. In the second section we show how to measure the quality based on chatbots' output. We check the grammar and spelling of the output. The last part of this notebook is dedicated to sentiment analysis that can be in many cases crucial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grammar and spelling\n",
    "\n",
    "There are several tools to check the spelling and grammar. We don't want our chatbot to reply with bad grammar or spelling errors. In Python we can use SpellChecker to check the spelling, pytypo to correct the typos and Language-check to check the grammar of a given sentence. We should check the grammar and spell so often as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spell checking\n",
    "\n",
    "Spell checking is one of the basic tool to check the output of our chatbot. It is not useful in many cases, only for a few generative-based chatbots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spellchecker import SpellChecker\n",
    "\n",
    "spell = SpellChecker()\n",
    "\n",
    "words = ['sample', 'words', 'heri', 'here']\n",
    "\n",
    "for word in words:\n",
    "    print(spell.correction(word))\n",
    "    print(spell.candidates(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Typos fixing\n",
    "\n",
    "We can also easily fix some simple typos with pytypo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytypo\n",
    "\n",
    "pytypo.correct_sentence('this traiining is great!!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grammar check\n",
    "\n",
    "A more complex tool that can measure the grammar is language tool that allows to check more than 25 languages. It's an app written in Java, but has ports in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import language_check\n",
    "\n",
    "tool = language_check.LanguageTool('en-US')\n",
    "\n",
    "tool.check(\"the are trainings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model quality measures\n",
    "\n",
    "Quality measure are more about the input and output of the model. We can take the dataset and depending on the way how we divided it, we can measure the quality of our model. The output can be also measured with some methods where the most popular is accuracy.\n",
    "\n",
    "### Dataset preparation\n",
    "\n",
    "One of the common problem that each data scientist has is about how to divide the data set into training and testing data sets. To understand the following equations we need to introduce new designations. Let $\\mathcal{L}_{n}$ be our training data set of size $n$, $T_{m}$ our testing data set of size $m$, $M_{e}$ the number of misclassified cases, $\\mathcal{I}$ a function that return 1 if there is a match between predicted and label value and $e(d)$ the error rate of classifier $d$. We use also $X$ and $Y$ sets that we have already explained. We can write the error rate like following:\n",
    "\\begin{equation}\n",
    "e(d)=\\frac{M_{e}}{m}.\n",
    "\\end{equation}\n",
    "It is the opposite to accuracy that is described later in this section. Error rate can be calculated differently depending on which method of data set preparation is used. There are few commonly used approached of how we can handle the training, testing and validation data sets:\n",
    "\n",
    "- resubstitution -- R-method,\n",
    "- hold-out -- H-method,\n",
    "- cross-validation -- $\\pi$-method,\n",
    "- bootstrap,\n",
    "- jackknife.\n",
    "\n",
    "The first method is a very simple one. We have the same data set for training and testing. It is not the best solution if we consider to have a solid classifier. The error rate can be written as following:\n",
    "\\begin{equation}\n",
    "e_{R}(d)=\\frac{1}{n}\\sum_{j=1}^{n}\\mathcal{I}(d(X_{j};\\mathcal{L}_{n})\\neq Y_{j}).\n",
    "\\end{equation}\n",
    "It means that we calculate the error rate for each element $j$ of our training data set and add 1 for each well predicted case. We need to divide it with $n$ which is the number of elements in the training data set. \n",
    "\n",
    "The second method is about dividing a data set into two data sets. It can be divided by half or other proportions. One set is our training data set and the second training data set. We can swap those sets and calculate the average of both sets. The error rate can be calculated as following:\n",
    "\\begin{equation}\n",
    "e_{\\tau}(\\hat{d})=\\frac{1}{m}\\sum_{j=1}^{m}I(\\hat{d}(X_{j}^{t};\\mathcal{L}_{n}\\neq Y_{j}^{t}).\n",
    "\\end{equation}\n",
    "Compared to resubstitution method it uses the testing data set only.\n",
    "Cross-validation is the most common approach. It can be also called as rotation method. We need to divide the data set into $k$ subsets. The elements in each set are randomly chosen. One of those sets are taken as a testing set where the other sets are merged into a  training set. It should be repeated $k$ times for each $k$ subset. The error rate can be calculated like following:\n",
    "\\begin{equation}\n",
    "e_{CV}(d)=\\frac{1}{n}\\sum_{j=1}^{n}I(\\hat{d}(X_{j};\\mathcal{L}_{n}^{(-j)}\\neq Y_{j}).\n",
    "\\end{equation}\n",
    "%sprawdzic n z m\n",
    "A special case is when $k=m$. It means that we have subsets where each consist of just one element. This approach is known as leave-one-out or U-method.\\\\\n",
    "Bootstrap method can be considered as an extension of resubstitution. The goal is to generate multiple sets from the main set by randomly selection. We use resubstitution method on each set and calculate an average error at the end:\n",
    "\\begin{equation}\n",
    "e_{B}(d)=\\frac{1}{B}\\sum_{b=1}^{B}\\frac{\\sum_{j=1}^{n}\\mathcal{I}(Z_{j}\\notin\\mathcal{L}_{n}^{\\star b})\\mathcal{I}(d(X_{j};\\mathcal{L}^{\\star b}_{n})\\not Y_{j})}{\\sum_{j=1}^{n}(Z_{j}\\notin\\mathcal{L}^{\\star b}_{n})}.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output quality metrics\n",
    "\n",
    "There are several metrics to show the quality of our classification model:\n",
    "\n",
    "- ROC that stands for Receiver Operating Characteristic curve,\n",
    "- AUC -- Area Under Curve,\n",
    "- $F_{1}$ score,\n",
    "- Precision,\n",
    "- Recall.\n",
    "\n",
    "To calculate the metrics we ned \n",
    "\n",
    "|                      |condition positive |condition negative |\n",
    "|----------------------|-------------------|-------------------|\n",
    "|**predicted positive**|True Positive (TP) |False Positive (FP)|         \n",
    "|**predicted negative**|False Negative (FN)|True Negative (TN) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most common metric is the accuracy. It can be calculated like following:\n",
    "\\begin{equation}\n",
    "ACC=\\frac{\\#TP+\\#TN}{\\#TP+\\#TN+\\#FP+\\#FN}.\n",
    "\\end{equation}\n",
    "First one that we describe is called True Positive Rate (TPR). It can be calculated like following:\n",
    "\\begin{equation}\n",
    "TPR=\\frac{\\#TP}{\\#TP+\\#FN}.\n",
    "\\end{equation}\n",
    "TPR is also called sensitivity or recall and is a measure of good predictions within a set of cases. By $\\#TP, \\#FP$ we mean the number of True Positive and False Positive cases. An opposite to it is specificity. It is also called TNR what stands for True Negative Rate. It can be calculated as following:\n",
    "\\begin{equation}\n",
    "TNR=\\frac{\\#TN}{\\#TN+\\#FP}.\n",
    "\\end{equation}\n",
    "It is a measure that says how good we are at predicting negative scenario. Another important metric is precision that is also known as Positive Predictive Value (PPV):\n",
    "\\begin{equation}\n",
    "PPV=\\frac{\\#TP}{\\#TP+\\#FP}.\n",
    "\\end{equation}\n",
    "It is a ratio of positive cases that that were well predicted to all positive cases, even those that are not well predicted. The opposite to it is the Negative Predictive Value:\n",
    "\\begin{equation}\n",
    "NPV=\\frac{TN}{TN+FN}.\n",
    "\\end{equation}\n",
    "We can also calculate the False Positive Rate metric known as fall-out. It is about how bad we are on predicting positive cases:\n",
    "\\begin{equation}\n",
    "FPR=1-TNR.\n",
    "\\end{equation}\n",
    "The opposite to FPR is False Negative Rate:\n",
    "\\begin{equation}\n",
    "FNR=1-TPR.\n",
    "\\end{equation}\n",
    "Another popular metric is called $F_{1}$ score and it is a weighted accuracy measure. It takes PPV and TPR to calculate the score:\n",
    "\\begin{equation}\n",
    "F_{1}=2\\frac{PPV\\cdot TPR}{TPR+PPV}.\n",
    "\\end{equation}\n",
    "The $F_{1}$ value as in case of all previous metrics between 1 and 0, where 1 is the best. \n",
    "A interesting measure is Matthews Correlation Coefficient measure that is about the correlation between observed and predicted values. The value of MCC is between -1 and 1. If we have a perfect classifier we get MCC=1. A random classifier is when we have MCC=0 and a totally bad classifier if have MCC=-1. This measure can be calculated as following:\n",
    "\\begin{equation}\n",
    "MCC=\\frac{\\#TP\\cdot\\#TN-\\#FP\\cdot\\#FN}{\\sqrt{(\\#TP+\\#FP)(\\#TP+\\#FN)(\\#TN+\\#FP)(\\#TN+\\#FN)}}.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_quality_metrics(data_set, predicted_set):\n",
    "    tn=0\n",
    "    tp=0\n",
    "    fn=0\n",
    "    fp=0\n",
    "    for i in range(len(data_set)):\n",
    "        if data_set[i]>0:\n",
    "            if data_set[i]==predicted_set[i]:\n",
    "                tp=tp+1\n",
    "            else:\n",
    "                fp=fp+1\n",
    "        else:\n",
    "            if data_set[i]==predicted_set[i]:\n",
    "                tn=tn+1\n",
    "            else:\n",
    "                fn=fn+1\n",
    "    acc=((tp+tn)*1.0)/((tp+tn+fp+fn)*1.0)\n",
    "    tpr=tp*1.0/(tp+fn)*1.0\n",
    "    tnr=tn*1.0/(tn+fp)*1.0\n",
    "    ppv=tp*1.0/(tp+fp)*1.0\n",
    "    npv=tn*1.0/(tn+fn)*1.0\n",
    "    fpr=1.0-tnr\n",
    "    fnr=1.0-tpr\n",
    "    f1=2*(ppv*tpr*1.0/(tpr+ppv*1.0))\n",
    "    return [acc,tpr,tnr,ppv,npv,fpr,fnr,f1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment analysis\n",
    "\n",
    "If we want to publish our chatbot on production, it's very important to measure the sentiment of the customers and our chatbot. We don't want to send to our customers a message with a negative sentiment. Two most popular libraries to check the sentiment analysis is CoreNLP and TextBlob. The libraries are trained on a dataset that usually does not give us the expected result. This is why many times we need to build our own library. Before we build a new one we check TextBlob to get the main idea of sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = \"The weather is good outside.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just get the sentiment for the example text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "text = TextBlob(example)\n",
    "text.sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A negative polarity means a negative sentiment, a posisivt polarity means a positive sentiment. The subjectivity means if the sentence is objective or subjective. The value is between 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Own sentiment analysis library\n",
    "\n",
    "We want to measure three different sentiments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SENTIMENT_TO_LABEL_MAPPING = {\n",
    "    \"negative\": -1,\n",
    "    \"neutral\": 0,\n",
    "    \"positive\": 1\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a classic example of airplane customers tweets. In the first place, we need to clean the tweets a bit to remove the emojis, duplicated and punctuation marks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import html\n",
    "\n",
    "# https://gist.github.com/Alex-Just/e86110836f3f93fe7932290526529cd1\n",
    "EMOJI_REGEX = re.compile(\"([\\U00010000-\\U0010ffff])\", re.UNICODE)\n",
    "DUPLICATED_LETTER_REGEX = re.compile(r\"([^a-z0-9])\\1+\", re.UNICODE | re.I)\n",
    "PUNCTUATION_MARKS_REGEX = re.compile(r\"([,\\.\\!\\?\\[\\]\\(\\)])\", re.UNICODE)\n",
    "LEADING_CHARACTER_REGEX = re.compile(r\"([^a-z0-9])([a-z0-9][^\\s]*)\", re.UNICODE | re.I)\n",
    "\n",
    "\n",
    "def preprocess_text(raw_text):\n",
    "    # Convert all the letters to lowercase\n",
    "    text = raw_text.lower()\n",
    "    # Remove html entities\n",
    "    text = html.unescape(text)\n",
    "    # Remove hashtag symbol and \"at\" for user mentions\n",
    "    text = text.replace(\"#\", \"\")\n",
    "    text = text.replace(\"@\", \"\")\n",
    "    # Divide the emojis written in a row with spaces\n",
    "    text = EMOJI_REGEX.sub(\"\\\\1 \", text)\n",
    "    # Remove quotation marks\n",
    "    text = text.replace(\"\\\"\", \"\")\n",
    "    text = text.replace(\"'\", \"\")\n",
    "    # Get rid of the misused spaces by\n",
    "    text = PUNCTUATION_MARKS_REGEX.sub(\" \\\\1 \", text)\n",
    "    # Divide leading special characters to a separate words\n",
    "    text = LEADING_CHARACTER_REGEX.sub(\"\\\\1 \\\\2\", text)\n",
    "    # Divide duplicated characters, so after text split they'll be treated\n",
    "    # as if they were a single character used a couple of times\n",
    "    text = DUPLICATED_LETTER_REGEX.sub(\"\\\\1\", text)\n",
    "    # Return preprocessed value\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before loading the tweets, make sure you have the dataset downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "raw_tweets = pd.read_csv(\"datasets/twitter-airlines-sentiment.csv\")\n",
    "\n",
    "# Preprocess the data with the function declared previously\n",
    "tweets = raw_tweets[[\"airline_sentiment\", \"text\"]]\n",
    "tweets.columns = (\"sentiment\", \"text\", )\n",
    "tweets[\"text\"] = tweets[\"text\"].map(preprocess_text)\n",
    "tweets[\"sentiment\"] = tweets[\"sentiment\"].map(lambda x: SENTIMENT_TO_LABEL_MAPPING[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use Random Forest as classifier, but we can use for it any other classifier. We check if with different division method and for different estimators.\n",
    "\n",
    "![random forest](images/random-forest.png)\n",
    "\n",
    "The code below can take some time to finish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "N_ESTIMATORS = (5, 10, 25, 50, 100)\n",
    "CRITERION = (\"gini\", \"entropy\")\n",
    "MAX_FEATURES = (\"auto\", \"log2\", None)\n",
    "\n",
    "# Divide the dataset into train and test fraction\n",
    "train_messages, test_messages, train_targets, test_targets = train_test_split(tweets[\"text\"], \n",
    "                                                                              tweets[\"sentiment\"],\n",
    "                                                                              test_size=0.2)\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "for n_estimators, criterion, max_features in itertools.product(N_ESTIMATORS,\n",
    "                                                               CRITERION,\n",
    "                                                               MAX_FEATURES):\n",
    "    # Define the classifier instance\n",
    "    classifier = RandomForestClassifier(random_state=2018, \n",
    "                                        n_estimators=n_estimators, \n",
    "                                        criterion=criterion, \n",
    "                                        max_features=max_features)\n",
    "    # Vectorize preprocessed sentences\n",
    "    train_features = vectorizer.fit_transform(train_messages)\n",
    "\n",
    "    # Train the model\n",
    "    %time fit = classifier.fit(train_features.toarray(), train_targets)\n",
    "\n",
    "    # Check the accuracy of the model on test data and display it\n",
    "    test_features = vectorizer.transform(test_messages)\n",
    "    train_predictions = fit.predict(train_features.toarray())\n",
    "    train_accuracy = accuracy_score(train_predictions, train_targets)\n",
    "    test_predictions = fit.predict(test_features.toarray())\n",
    "    test_accuracy = accuracy_score(test_predictions, test_targets)\n",
    "    print(\"Configuration: n_estimators = {}, criterion = {}, max_features = {}\\n\"\n",
    "          \"Train accuracy score: {}\\n\"\n",
    "          \"Test accuracy score: {}\\n\".format(n_estimators, criterion, max_features, \n",
    "                                             train_accuracy, test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turned out the following configuration achieves the best accuracy on our test dataset:\n",
    "\n",
    "n_estimators = 100, criterion = gini, max_features = auto\n",
    "\n",
    "For that reason we are going to create a simple application that will use these parameters for training. That will be a console application reading the sentences from the user and classifies its sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature importance\n",
    "\n",
    "One of the biggest advantages of Random Forest classifier is the ability to describe the importance of the used features. It allows to check which variables have the best predictive force and to understand how the model performs the decision. The following code snippet visualizes the feature importance for our created model:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
